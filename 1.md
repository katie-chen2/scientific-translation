# Overview of Normalization
## Why Normalization is Needed

In the field of machine learning, there is a very important assumption known as independent and identically distributed (i.i.d.), which assumes that the training data and test data are from the same distribution. This is the basis for models trained on the training set to perform well on the test set. Data that are independent and identically distributed can simplify model training and improve the model's learning ability, which is basically a consensus.

However, if the distribution of the data \((X, Y)\) changes, not meeting the i.i.d. constraint, this problem is known as covariate shift. The models learned in this way will perform poorly when faced with constantly changing inputs.

This problem does not only arise when the distribution between the training and test phases is inconsistent. During the training phase, for deep learning models composed of many layers, the changes in the parameters of each layer will continuously change the input distribution of that layer, facing the covariate shift problem at every level. Since this problem occurs in the hidden layers inside the network, it is referred to as the Internal Covariate Shift (ICS) problem.

The distribution of input data for each hidden layer is constantly changing, and after stacking through layers, it leads to very drastic changes in the input distribution of the higher layers, forcing the higher layers to continuously adapt to the parameter updates of the lower layers.

ICS leads to the inputs of each neuron no longer being independent and identically distributed, causing:

- The parameters of the higher layers need to continuously adapt to the new input data distribution, slowing down the training speed.
- The changes in the inputs of the lower layers may tend to increase or decrease, causing the higher layers to fall into the saturation zone, making the learning stop prematurely, or even not converge at all.
- The updates of each layer will affect other layers, so the parameter update strategy of each layer needs to be as cautious as possible.

Simply put, it can lead to slow network convergence, or even difficulty in converging. And the capability of the obtained model is also affected.

Normalization aims to solve the problem of ICS, stabilizing the distribution of activation inputs for each hidden layer neuron.

